# Handle missing device types and clean hierarchy
df_clean = df.dropna(subset=['device_type']).copy()

# Create treemap with clean data
device_fig = px.treemap(
    df_clean,
    path=[px.Constant('All Devices'), 'device_type', 'os_name', 'os_version'],
    title='Device Type > OS > Version Hierarchy (Cleaned Data)'
)
device_fig.update_traces(root_color="lightgrey")
device_fig.show()

# Create treemap with clean data
device_fig_v2 = px.treemap(
    df_clean,
    path=[px.Constant('All Devices'), 'os_version', 'os_name', 'device_type'],
    title='Device Type > OS > Version Hierarchy (Cleaned Data) V2',
    # hover_data=['device_type', 'os_name', 'os_version', 'event_slug'],
)
device_fig_v2.update_traces(root_color="lightgrey")
device_fig_v2.show()







# Event Types Metadata
print("Before splitting by : delimiter")
num_event_types = df['event_type'].nunique()
event_types = df['event_type'].unique()
print(f"Number of unique event types: {num_event_types}")

print("\n After splitting by : delimiter")

df_event_types = df.copy()

# Split the 'event_type' column by one or more colons (:)    AND IGNORE WHEN COLON IS AT THE BEGINNING
df_event_types['event_type'] = df_event_types['event_type'].str.lstrip(':')
split_event_types = df_event_types['event_type'].str.split(r':+', expand=True)



# Get the number of columns after the split
print(f"Split event_types into {split_event_types.shape[1]} parts")

# Rename the columns (update based on expected number of parts)
split_event_types.columns = [f'event_type_lvl_{i}' for i in range(split_event_types.shape[1])]
# split_event_types = split_event_types.fillna('None')
split_event_types = split_event_types.replace('', 'None').fillna('None')

print(f"Types of level 0 event types: {split_event_types['event_type_lvl_0'].unique()}")
# print(f"Types of level 1 event types: {split_event_types['event_type_lvl_1'].unique()}")
# print(f"Types of level 2 event types: {split_event_types['event_type_lvl_2'].unique()}")
# print(f"Types of level 3 event types: {split_event_types['event_type_lvl_3'].unique()}")







# Aside: I wonder if the unique values in event_type_lvl_0 to event_type_lvl_3 are mutually exclusive?
# Get distinct values from each of the 4 columns
distinct_values_lvl_0 = set(split_event_types['event_type_lvl_0'].unique())
distinct_values_lvl_1 = set(split_event_types['event_type_lvl_1'].unique())
distinct_values_lvl_2 = set(split_event_types['event_type_lvl_2'].unique())
distinct_values_lvl_3 = set(split_event_types['event_type_lvl_3'].unique())

# Check intersections between all pairs of sets
intersection_1_2 = distinct_values_lvl_0 & distinct_values_lvl_1
intersection_1_3 = distinct_values_lvl_0 & distinct_values_lvl_2
intersection_1_4 = distinct_values_lvl_0 & distinct_values_lvl_3
intersection_2_3 = distinct_values_lvl_1 & distinct_values_lvl_2
intersection_2_4 = distinct_values_lvl_1 & distinct_values_lvl_3
intersection_3_4 = distinct_values_lvl_2 & distinct_values_lvl_3

# Store intersections and corresponding set names
intersections = {
    "lvl_0 & lvl_1": intersection_1_2,
    "lvl_0 & lvl_2": intersection_1_3,
    "lvl_0 & lvl_3": intersection_1_4,
    "lvl_1 & lvl_2": intersection_2_3,
    "lvl_1 & lvl_3": intersection_2_4,
    "lvl_2 & lvl_3": intersection_3_4
}

# Check if there are any intersections
mutually_exclusive = not any(intersections.values())

# Output result
if mutually_exclusive:
    print("The columns are mutually exclusive (no overlap of distinct values).")
else:
    print("The columns are not mutually exclusive (there is overlap of distinct values).")
    for pair, intersection in intersections.items():
        if intersection:
            print(f"Intersection between {pair}: {intersection}")






print(split_event_types[split_event_types['event_type_lvl_0'] == 'all-accounts'].head(1))
print(split_event_types[split_event_types['event_type_lvl_1'] == 'all-accounts'].head(1))

print(split_event_types[split_event_types['event_type_lvl_1'] == 'widget'].head(1))
print(split_event_types[split_event_types['event_type_lvl_2'] == 'widget'].head(1))




# Plot the treemap to explain event types hierarchy
device_fig = px.treemap(
    split_event_types,
    path=['event_type_lvl_0', 'event_type_lvl_1', 'event_type_lvl_2', 'event_type_lvl_3'],
    title='Event Types Hierarchy'
)
device_fig.update_traces(root_color="lightgrey")
device_fig.show()







import plotly.graph_objects as go
# Copy the dataframe
df_event_types = df.copy()

# Split the 'event_type' column by one or more colons (:) using regular expression
split_event_types = df_event_types['event_type'].str.split(r':+', expand=True)

# Get the number of columns after the split
print(f"Split event_types into: {split_event_types.shape[1]} parts")

# Rename the columns (update based on expected number of parts)
split_event_types.columns = [f'event_type_lvl_{i}' for i in range(split_event_types.shape[1])]
split_event_types = split_event_types.fillna('None')

# Add the split columns to the copied DataFrame
df_event_types = pd.concat([df_event_types, split_event_types], axis=1)

# Create a list of unique nodes from the event type levels
nodes = list(pd.concat([df_event_types['event_type_lvl_0'], df_event_types['event_type_lvl_1'], df_event_types['event_type_lvl_2'], df_event_types['event_type_lvl_3']]).unique())

# Map the event type levels to node indices
df_event_types['event_type_lvl_0_idx'] = df_event_types['event_type_lvl_0'].apply(lambda x: nodes.index(x))
df_event_types['event_type_lvl_1_idx'] = df_event_types['event_type_lvl_1'].apply(lambda x: nodes.index(x))
df_event_types['event_type_lvl_2_idx'] = df_event_types['event_type_lvl_2'].apply(lambda x: nodes.index(x))
df_event_types['event_type_lvl_3_idx'] = df_event_types['event_type_lvl_3'].apply(lambda x: nodes.index(x))

# Count the occurrences between each level
link_data = [
    {'source': df_event_types['event_type_lvl_0_idx'], 'target': df_event_types['event_type_lvl_1_idx']},
    {'source': df_event_types['event_type_lvl_1_idx'], 'target': df_event_types['event_type_lvl_2_idx']},
    {'source': df_event_types['event_type_lvl_2_idx'], 'target': df_event_types['event_type_lvl_3_idx']},
]

# Count the occurrences for each link
links = []
for data in link_data:
    link_counts = pd.Series(data['source']).value_counts().to_dict()
    for source_idx, count in link_counts.items():
        target_idx = data['target'][data['source'] == source_idx].iloc[0]
        links.append({
            'source': source_idx,
            'target': target_idx,
            'value': count
        })

# Create the Sankey diagram
fig = go.Figure(go.Sankey(
    node=dict(
        pad=15,
        thickness=20,
        line=dict(color="black", width=0.5),
        label=nodes
    ),
    link=dict(
        source=[link['source'] for link in links],
        target=[link['target'] for link in links],
        value=[link['value'] for link in links]
    )
))

fig.update_layout(title_text="Event Types Hierarchy", font_size=10)
fig.show()




# 2. Event Type Frequency
event_counts = df['event_type'].value_counts().nlargest(15)
plt.figure(figsize=(12,6))
event_counts.plot(kind='barh', color='teal')
plt.title('Top 15 Event Types')
plt.xlabel('Count')
plt.gca().invert_yaxis()
plt.show()



# 3. Geographic Distribution
geo_counts = df['country'].value_counts().nlargest(10)
px.bar(geo_counts, orientation='h', 
       title='Top 10 Countries by Event Count',
       labels={'value': 'Count', 'index': 'Country'}).show()




# 4. User Behavior Over Time
# Ensure datetime conversion and add time features
df['event_time'] = pd.to_datetime(df['event_time'])
df['hour'] = df['event_time'].dt.floor('h')

# Create time-based aggregation with event counting
hourly_activity = df.groupby('hour').size().reset_index(name='count')

# Create interactive time series plot
timeline_fig = px.line(
    hourly_activity,
    x='hour',
    y='count',
    title='User Activity Timeline',
    labels={'count': 'Number of Events', 'hour': 'Time'}
)
timeline_fig.update_xaxes(rangeslider_visible=True)
timeline_fig.show()










# 6. UTM Source Effectiveness
# Clean UTM data and handle missing values
utm_data = df[df['utm_source'].str.strip().isin(['EMPTY', '']) == False]
utm_data = utm_data.dropna(subset=['utm_source', 'event_type'])

if not utm_data.empty:
    px.sunburst(
        utm_data,
        path=['utm_source', 'event_type'],
        title='UTM Source Effectiveness (Filtered Data)',
        color_discrete_sequence=px.colors.qualitative.Pastel
    ).show()
else:
    print("No UTM data available after cleaning")










# 7. Session Analysis
# Calculate session duration in minutes
session_analysis['duration_min'] = session_analysis['duration'].dt.total_seconds()/60

# Create session duration distribution plot
px.histogram(
    session_analysis,
    x='duration_min',
    nbins=50,
    title='Session Duration Distribution',
    labels={'duration_min': 'Duration (minutes)'}
).show()
session_analysis['duration'] = session_analysis['end_time'] - session_analysis['start_time']
